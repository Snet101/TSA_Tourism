{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a32bcf",
   "metadata": {},
   "source": [
    "# Appendix B:  Data Processing Pipeline (Code Listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5433124",
   "metadata": {},
   "source": [
    "## B1. Setup, Imports, and Project Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pytrends.request import TrendReq\n",
    "import requests\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #for clean output and keeps notebook clean\n",
    "from pathlib import Path\n",
    "\n",
    "# Project folders (relative to this notebook's working directory)\n",
    "DATA_RAW = Path(\"data_raw\")\n",
    "DATA_CLEAN = Path(\"data_clean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009743d",
   "metadata": {},
   "source": [
    "## B2. Washington, DC; Combine Yearly Files into One Raw CSV\n",
    "\n",
    "Purpose: merge multiple DC CSV extracts into a single raw file for cleaning. (Includes a quick column preview.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_files = sorted(DATA_RAW.glob(\"dc_*.csv\"))\n",
    "print(\"DC files:\", dc_files)\n",
    "\n",
    "dc_all = pd.concat([pd.read_csv(f) for f in dc_files], ignore_index=True)\n",
    "\n",
    "dc_all.to_csv(DATA_RAW / \"washington_dc_311_raw.csv\", index=False)\n",
    "\n",
    "print(\"DC combined shape:\", dc_all.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_RAW / \"washington_dc_311_raw.csv\", nrows=5) #to check column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b8f69",
   "metadata": {},
   "source": [
    "## B3. Boston; Merge Yearly Raw Files\n",
    "\n",
    "Purpose: combine Boston yearly CSVs into a single raw dataset and define output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BOSTON_DIR = Path(\"data/raw/boston_yearly\")\n",
    "out_raw_path = Path(\"data/raw/311_boston_raw_2019_2025.csv\")\n",
    "out_clean_path = Path(\"data/clean/311_boston_2019_2025.csv\")\n",
    "\n",
    "def merge_boston_yearly_csvs(folder: Path) -> pd.DataFrame:\n",
    "    files = sorted(folder.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {folder.resolve()}\")\n",
    "\n",
    "    parts = []\n",
    "    for f in files:\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # to keep track of which file it came from\n",
    "        df[\"source_file\"] = f.name\n",
    "\n",
    "        parts.append(df)\n",
    "\n",
    "        print(f\"Loaded {f.name}: {len(df):,} rows\")\n",
    "\n",
    "    merged = pd.concat(parts, ignore_index=True)\n",
    "    print(f\"\\nMerged total: {len(merged):,} rows\")\n",
    "    return merged\n",
    "\n",
    "# 1) Merge raw yearly files\n",
    "boston_raw = merge_boston_yearly_csvs(DATA_RAW)\n",
    "\n",
    "# 2) Save the merged raw (to never have to re-merge)\n",
    "out_raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "boston_raw.to_csv(out_raw_path, index=False)\n",
    "\n",
    "# 3) Clean + standardize to your canonical schema\n",
    "BOSTON_DATE_COL = \"open_dt\"\n",
    "BOSTON_TYPE_COL = \"type\"\n",
    "\n",
    "boston_clean = boston_raw.copy()\n",
    "boston_clean[\"date\"] = pd.to_datetime(boston_clean[BOSTON_DATE_COL], errors=\"coerce\")\n",
    "boston_clean[\"complaint_type\"] = boston_clean[BOSTON_TYPE_COL].astype(\"string\")\n",
    "boston_clean[\"city\"] = \"Boston\"\n",
    "\n",
    "boston_clean = boston_clean[[\"city\", \"date\", \"complaint_type\"]].dropna(subset=[\"date\", \"complaint_type\"])\n",
    "boston_clean = boston_clean[boston_clean[\"complaint_type\"].str.len() > 0]\n",
    "boston_clean = boston_clean[(boston_clean[\"date\"] >= \"2019-01-01\") & (boston_clean[\"date\"] < \"2026-01-01\")]\n",
    "\n",
    "# 4) Save clean\n",
    "out_clean_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "boston_clean.to_csv(out_clean_path, index=False)\n",
    "\n",
    "# 5) Sanity check year coverage\n",
    "print(\"\\nBoston years:\", boston_clean[\"date\"].dt.year.value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fd414",
   "metadata": {},
   "source": [
    "## B4. City-Level Cleaning Examples\n",
    "\n",
    "Purpose: apply a standardized cleaning function to raw city-level 311 datasets to improve formatting and write cleaned outputs (Seattle and Washington, DC examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_raw = pd.read_csv(\"data_raw/washington_dc_311_raw.csv\")  \n",
    "dc_clean = clean_311(dc_raw, \"Washington DC\", \"ADDDATE\", \"SERVICECODEDESCRIPTION\")\n",
    "dc_clean.to_csv(\"data_clean/311_washington_dc_2019_2025.csv\", index=False)\n",
    "print(dc_clean[\"date\"].dt.year.value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sea_raw = pd.read_csv(\"data_raw/seattle_2018_2025_raw.csv\", nrows=5)\n",
    "print(sea_raw.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sea_raw = pd.read_csv(\"data_raw/seattle_2018_2025_raw.csv\")\n",
    "\n",
    "sea_clean = clean_311(\n",
    "    sea_raw,\n",
    "    city=\"Seattle\",\n",
    "    date_col=\"Created Date\",\n",
    "    type_col=\"Service Request Type\"\n",
    ")\n",
    "\n",
    "sea_clean.to_csv(\"data_clean/311_seattle_2019_2025.csv\", index=False)\n",
    "\n",
    "# sanity check\n",
    "print(sea_clean[\"date\"].dt.year.value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bb289",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.read_csv(\"data_clean/dc_311_clean.csv\")\n",
    "\n",
    "dc = dc.rename(columns={\n",
    "    \"created_date\": \"date\",\n",
    "    \"service_request_type\": \"complaint_type\"\n",
    "})\n",
    "\n",
    "dc = std(dc, \"Washington DC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24d7d5",
   "metadata": {},
   "source": [
    "## B5. Standardize Cleaned City Files and Combine into One 311 Table\n",
    "\n",
    "Purpose: load cleaned city files, standardize columns, and concatenate into a unified 311 dataframe (`df311`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8effaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sea = pd.read_csv(\"data_clean/seattle_311_clean.csv\")\n",
    "bos = pd.read_csv(\"data_clean/boston_311_clean.csv\")\n",
    "dc  = pd.read_csv(\"data_clean/dc_311_clean.csv\")\n",
    "\n",
    "def std(df, city_name):\n",
    "    df = df.copy()\n",
    "    df[\"city\"] = city_name if \"city\" not in df.columns else df[\"city\"]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"complaint_type\"] = df[\"complaint_type\"].astype(str)\n",
    "    return df.dropna(subset=[\"date\"])\n",
    "\n",
    "sea = std(sea, \"Seattle\")\n",
    "bos = std(bos, \"Boston\")\n",
    "dc  = std(dc, \"Washington DC\")\n",
    "\n",
    "df311 = pd.concat([sea, bos, dc], ignore_index=True)\n",
    "df311.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0442f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload from disk \n",
    "sea_raw = pd.read_csv(\"data_clean/seattle_311_clean.csv\")\n",
    "bos_raw = pd.read_csv(\"data_clean/boston_311_clean.csv\")\n",
    "dc_raw  = pd.read_csv(\"data_clean/dc_311_clean.csv\")\n",
    "\n",
    "def std_basic(df, city_name):\n",
    "    df = df.copy()\n",
    "    df[\"city\"] = city_name\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"complaint_type\"] = df[\"complaint_type\"].astype(str)\n",
    "    return df.dropna(subset=[\"date\"])[[\"city\",\"date\",\"complaint_type\"]]\n",
    "\n",
    "# Seattle + Boston are already standardized\n",
    "sea = std_basic(sea_raw, \"Seattle\")\n",
    "bos = std_basic(bos_raw, \"Boston\")\n",
    "\n",
    "# DC: detect/rename columns if needed\n",
    "dc = dc_raw.copy()\n",
    "dc.columns = [c.strip() for c in dc.columns]\n",
    "\n",
    "# If DC already has date/complaint_type, this keeps them.\n",
    "# If not, rename common alternatives:\n",
    "rename_map = {}\n",
    "if \"date\" not in dc.columns:\n",
    "    for cand in [\"created_date\", \"creation_date\", \"requested_datetime\", \"request_date\", \"open_date\"]:\n",
    "        if cand in dc.columns:\n",
    "            rename_map[cand] = \"date\"\n",
    "            break\n",
    "if \"complaint_type\" not in dc.columns:\n",
    "    for cand in [\"sr_type\", \"service_name\", \"service_type\", \"request_type\", \"type\", \"category\"]:\n",
    "        if cand in dc.columns:\n",
    "            rename_map[cand] = \"complaint_type\"\n",
    "            break\n",
    "\n",
    "if rename_map:\n",
    "    dc = dc.rename(columns=rename_map)\n",
    "\n",
    "# Now standardize DC\n",
    "dc = std_basic(dc, \"Washington DC\")\n",
    "\n",
    "# Combine\n",
    "df311 = pd.concat([sea, bos, dc], ignore_index=True)\n",
    "df311[\"date\"] = pd.to_datetime(df311[\"date\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "\n",
    "print(\"df311 cities:\", df311[\"city\"].value_counts())\n",
    "print(\"df311 date range:\", df311[\"date\"].min(), \"→\", df311[\"date\"].max())\n",
    "\n",
    "df311.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77ce5b",
   "metadata": {},
   "source": [
    "## B6. Define Tourism Keywords\n",
    "\n",
    "Purpose: regex keyword list used to flag tourism-related complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOURISM_KEYWORDS = [\n",
    "    # crowd related\n",
    "    r\"\\bnoise\\b\", r\"\\bloud\\b\", r\"\\bparty\\b\", r\"\\bnight\\b\", r\"\\bbar\\b\", r\"\\bclub\\b\",\n",
    "    r\"\\bmusic\\b\", r\"\\bconcert\\b\", r\"\\bcrowd\\b\", r\"\\bovercrowd(ed|ing)?\\b\",\n",
    "    r\"\\btour\\s?group\\b\",\n",
    "\n",
    "    # aircraft related\n",
    "    r\"\\bairplane\\b\", r\"\\baircraft\\b\", r\"\\bflight\\b\",\n",
    "    r\"\\bairport\\b\", r\"\\bjet\\b\", r\"\\bjet\\s?noise\\b\",\n",
    "    r\"\\blanding\\b\", r\"\\btakeoff\\b\", r\"\\bflight\\s?path\\b\",\n",
    "\n",
    "    # housing related\n",
    "    r\"\\bairbnb\\b\", r\"\\bshort\\s?term\\s?rental\\b\", r\"\\bvacation\\s?rental\\b\",\n",
    "    r\"\\bhotel\\b\", r\"\\bmotel\\b\", r\"\\bguest\\s?house\\b\",\n",
    "    r\"\\bloud\\s?guests\\b\", r\"\\bparty\\s?house\\b\",\n",
    "\n",
    "    # public space related\n",
    "    r\"\\blitter\\b\", r\"\\btrash\\b\", r\"\\bgarbage\\b\", r\"\\brat(s)?\\b\",\n",
    "    r\"\\bpublic\\s?restroom\\b\", r\"\\btoilet\\b\", r\"\\bsanitation\\b\",\n",
    "    r\"\\boverflow(ing)?\\b\", r\"\\bbins?\\b\",\n",
    "\n",
    "    # vehicle related\n",
    "    r\"\\bparking\\b\", r\"\\btraffic\\b\", r\"\\bcongestion\\b\",\n",
    "    r\"\\bbus(es)?\\b\", r\"\\bmetro\\b\", r\"\\bsubway\\b\", r\"\\btrain\\b\",\n",
    "    r\"\\btaxi\\b\", r\"\\buber\\b\", r\"\\blyft\\b\", r\"\\brideshare\\b\",\n",
    "    r\"\\btour\\s?bus\\b\",\n",
    "\n",
    "    # activity related\n",
    "     r\"\\bbeach\\b\", r\"\\btour\\b\", r\"\\btourist(s)?\\b\", r\"\\bvisitor(s)?\\b\",\n",
    "    r\"\\bevent\\b\", r\"\\bfestival\\b\", r\"\\bparade\\b\",\n",
    "    r\"\\bstadium\\b\", r\"\\battraction\\b\", r\"\\blandmark\\b\",\n",
    "]\n",
    "\n",
    "pattern = re.compile(\"|\".join(TOURISM_KEYWORDS), flags=re.IGNORECASE)\n",
    "\n",
    "df311[\"is_tourism_related\"] = df311[\"complaint_type\"].apply(lambda s: bool(pattern.search(s)))\n",
    "df311[[\"complaint_type\", \"is_tourism_related\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75e3f86",
   "metadata": {},
   "source": [
    "## B7. Flag Tourism Complaints and Aggregate to Daily and Monthly Metrics\n",
    "\n",
    "Purpose: apply keyword filter, create daily counts, then aggregate daily → monthly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applied keyword filter\n",
    "pattern = re.compile(\"|\".join(TOURISM_KEYWORDS), flags=re.IGNORECASE)\n",
    "df311[\"is_tourism_related\"] = df311[\"complaint_type\"].apply(lambda s: bool(pattern.search(s)))\n",
    "\n",
    "print(df311[\"is_tourism_related\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure date is datetime (column name)\n",
    "df311[\"date\"] = pd.to_datetime(df311[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# optional sanity checks\n",
    "print(df311[\"date\"].dtype)\n",
    "print(\"NaT count:\", df311[\"date\"].isna().sum())\n",
    "\n",
    "# drop rows where date failed to parse\n",
    "df311 = df311.dropna(subset=[\"date\"])\n",
    "\n",
    "daily_tourism = (\n",
    "    df311\n",
    "    .assign(day=lambda d: d[\"date\"].dt.date)\n",
    "    .groupby([\"city\", \"day\"])\n",
    "    .agg(\n",
    "        n_311_calls=(\"complaint_type\", \"size\"),\n",
    "        n_tourism_calls=(\"is_tourism_related\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "daily_tourism[\"tourism_call_share\"] = daily_tourism[\"n_tourism_calls\"] / daily_tourism[\"n_311_calls\"]\n",
    "print(sorted(daily_tourism[\"city\"].unique()))\n",
    "\n",
    "daily_tourism.to_csv(\n",
    "    \"data_clean/tourism_311_daily_sea_bos_dc.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Saved: tourism_311_daily_sea_bos_dc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert daily to monthly\n",
    "daily_tourism[\"day\"] = pd.to_datetime(daily_tourism[\"day\"], errors=\"coerce\")\n",
    "\n",
    "monthly_311 = (\n",
    "    daily_tourism\n",
    "    .dropna(subset=[\"day\"])\n",
    "    .assign(month=lambda d: d[\"day\"].dt.to_period(\"M\").dt.to_timestamp())\n",
    "    .groupby([\"city\", \"month\"])\n",
    "    .agg(\n",
    "        n_311_calls=(\"n_311_calls\", \"sum\"),\n",
    "        n_tourism_calls=(\"n_tourism_calls\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "monthly_311[\"tourism_call_share\"] = monthly_311[\"n_tourism_calls\"] / monthly_311[\"n_311_calls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(daily_tourism[\"city\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ef2df9",
   "metadata": {},
   "source": [
    "## B8. Load Google Trends and Merge with Monthly 311 Metrics\n",
    "\n",
    "Purpose: load city-month tourism intensity files and merge with monthly 311 burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load google trends\n",
    "sea_trends = pd.read_csv(\"data_clean/seattle_tourism_monthly.csv\")\n",
    "bos_trends = pd.read_csv(\"data_clean/boston_tourism_monthly.csv\")\n",
    "dc_trends  = pd.read_csv(\"data_clean/washington_dc_tourism_monthly.csv\")\n",
    "\n",
    "trends = pd.concat([sea_trends, bos_trends, dc_trends], ignore_index=True)\n",
    "trends[\"month\"] = pd.to_datetime(trends[\"month\"], errors=\"coerce\")\n",
    "trends = trends.dropna(subset=[\"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging 311 data and google trends\n",
    "final_monthly = monthly_311.merge(\n",
    "    trends,\n",
    "    on=[\"city\", \"month\"],\n",
    "    how=\"inner\"\n",
    ").sort_values([\"city\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "final_monthly.to_csv(\n",
    "    \"data_clean/final_monthly_311_plus_trends.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Saved: final_monthly_311_plus_trends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"monthly_311 cities:\", sorted(monthly_311[\"city\"].unique()))\n",
    "print(\"trends cities:\", sorted(trends[\"city\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checks\n",
    "print(final_monthly.groupby(\"city\")[\"month\"].agg([\"min\", \"max\", \"count\"]))\n",
    "final_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e359bbd",
   "metadata": {},
   "source": [
    "## B.9 Monthly Aggregated Complaint Metrics (Sample Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5fe62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_monthly[\n",
    "    (final_monthly[\"city\"] == \"Boston\") &\n",
    "    (final_monthly[\"month\"].dt.year == 2019)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a34bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_monthly[\n",
    "    (final_monthly[\"city\"] == \"Seattle\") &\n",
    "    (final_monthly[\"month\"].dt.year == 2019)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63aaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_monthly[\n",
    "    (final_monthly[\"city\"] == \"Washington DC\") &\n",
    "    (final_monthly[\"month\"].dt.year == 2019)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733cdfe",
   "metadata": {},
   "source": [
    "## B10. Visualization and Correlation Checks\n",
    "\n",
    "Purpose: create diagostic plots and a correlation table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e836914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for city in final_monthly[\"city\"].unique():\n",
    "    sub = final_monthly[final_monthly[\"city\"] == city]\n",
    "    plt.figure()\n",
    "    plt.scatter(sub[\"tourism_intensity\"], sub[\"tourism_call_share\"])\n",
    "    plt.title(f\"{city}: Tourism Intensity vs Tourism 311 Share\")\n",
    "    plt.xlabel(\"Tourism Intensity (Google Trends)\")\n",
    "    plt.ylabel(\"Tourism-related 311 Share\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in final_monthly[\"city\"].unique():\n",
    "    sub = final_monthly[final_monthly[\"city\"] == city].sort_values(\"month\")\n",
    "    plt.figure()\n",
    "    plt.plot(sub[\"month\"], sub[\"tourism_call_share\"])\n",
    "    plt.title(f\"{city}: Tourism-related 311 Share Over Time\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Tourism-related 311 Share\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977df062",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_table = (\n",
    "    final_monthly\n",
    "    .groupby(\"city\")[[\"tourism_intensity\", \"tourism_call_share\"]]\n",
    "    .corr()\n",
    ")\n",
    "corr_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
